{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FdDZOMab65p1iZKLJwcffKswq_yXR2lq","timestamp":1677469700087}],"authorship_tag":"ABX9TyMEM3Cm6B0VlSkWTA/eTCD7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Write a program in Python to implement value iteration, policy iteration, and modified policy iteration\n","specifically for this simple MDP example.\n","For this matter, you should start by creating a simple MDP class using class MDP. This class should\n","include the following members:\n","▪ a constructor for the MDP class def __init()__ that has the following parameters: self,\n","T, R, discount. \n","o T -- Transition function: |A| x |S| x |S'| array\n","o R -- Reward function: |A| x |S| array\n","o discount -- discount factor γ: scalar in [0,1)\n","The constructor should verify that the inputs are valid (using the assert command) and set\n","corresponding variables in an MDP object.\n","▪ a procedure for the value iteration def valueIteration() that has the following\n","parameters: self, initialV , nIterations, tolerance.\n","Set nIterations and tolerance to np.inf and 0.01 as default values, respectively.\n","o initialV -- Initial value function: array of |S| entries\n","o nIterations -- limit on the number of iterations: scalar (default: infinity)\n","o tolerance -- threshold on ‖𝑉𝑛 − 𝑉𝑛+1‖∞ that will be compared to a variable epsilon\n","(initialized to np.inf): scalar (default: 0.01)\n","This procedure should return a new value function V.\n","o newV – New value function: array of |S| entries.\n","o iteration – the number of iterations performed: scalar\n","o epsilon -- ‖𝑉𝑛 − 𝑉𝑛+1‖∞: scalar"],"metadata":{"id":"fwk_cjaTyUDc"}},{"cell_type":"code","source":["#imports\n","import numpy as np\n","import pandas as pd\n","import sys\n"],"metadata":{"id":"ZlQbPNg91mfv","executionInfo":{"status":"ok","timestamp":1677468091030,"user_tz":360,"elapsed":545,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#rough\n","temp= np.array([[1,2,3],[4,5,6]])\n","isinstance(temp, np.ndarray)\n","print(temp.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6F950spC12ra","executionInfo":{"status":"ok","timestamp":1677439004248,"user_tz":360,"elapsed":180,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"e6b9c07d-a3b4-489f-df72-c43eb0316026"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"r8FE-OJIxIMc","executionInfo":{"status":"ok","timestamp":1677468093857,"user_tz":360,"elapsed":178,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}}},"outputs":[],"source":["\n","class MDP:\n","  def __init__(self,T,R,discount):\n","    assert isinstance(T, np.ndarray) and isinstance(R, np.ndarray), \"The transition and reward array should be numpy arrays\"\n","    assert discount >0 and discount <= 1, \"The discount factor should be scalar in [0,1)\"\n","    assert (R.shape[0] == T.shape[1] and T.shape[1] == T.shape[2]), \"Invalid shape of input array\"#dimension check for R and T\n","    self.T = T # Transition probability T is a 3 dimensional matrix which contains both TA and TS\n","    self.R = R\n","    self.discount = discount\n","  \n","  def valueFunction(self, oldV,getMax=False):\n","    newV = self.R + self.discount*np.dot(self.T,oldV) #Belmans equation\n","    policy=np.argmax(newV,axis=0)\n","    VA,VS= newV\n","    if(not getMax):\n","      return newV\n","    maxV= np.maximum(VA,VS)\n","    return newV,maxV\n","\n","  #NOTE: The policy determines which transition matrix (for probability distribution) should be taken.\n","  #for each state we have action A and S. The transition probability for the policy will depend\n","  # on the action we take on that state. T=> for a state T[A] or T[S]?\n","  def getTransitionForPolicy(self,policy):\n","    newT = []\n","    for ind,item in enumerate(policy):\n","      newT.append(self.T[item[0]][ind]) #chosing transition matrix according to policy\n","    newT=np.array(newT)\n","    return newT;\n","  \n","  def valueIteration( self, initialV , nIterations = np.inf, tolerance = 0.01):\n","    assert isinstance(initialV, np.ndarray), \"The intial value function array should be numpy arrays\"\n","    assert self.R.shape[0] == initialV.shape[0], \"Invalid shape of input array\"\n","\n","    n = 0\n","    epsilon = float('inf')\n","    oldV = initialV\n","    maxV= None\n","    newV= None\n","    while(n< nIterations and epsilon > tolerance):\n","      newV,maxV= self.valueFunction(oldV,True)\n","      epsilon = np.linalg.norm(maxV - oldV)\n","      oldV=maxV\n","      n+=1\n","      print(f'iteration {n} \\n value iteration inside loop new value : {newV} \\n, max value: {maxV} \\n, epsilon: {epsilon}')\n","    return newV,n,epsilon\n","  \n","  def extractPolicy(self, V):\n","    policy=np.argmax(V,axis=0)\n","    #NOTE: 0 represents A and 1 represents S\n","    return policy \n","\n","  def evaluatePolicy(self, policy):\n","    #MAIN FORMULA : V = R + gamma*T*V, V= R*(I-gamma*T))^-1\n","    newT = self.getTransitionForPolicy(policy)\n","    I = np.identity(self.T[0].shape[0])  #The identity matrix should be the shape as V. The V will have same shape as R. But as we need to substract the matrix with T\n","    tempInv = np.linalg.inv((I-self.discount*newT))\n","    newV = np.dot(tempInv,self.R)\n","    return newV\n","\n","  def improvePolicy(self, oldV):\n","    newV= self.valueFunction(oldV)\n","    policy = self.extractPolicy(newV)\n","    return policy\n","\n","\n","\n","\n","  #2 steps, 1) evaluate policy 2) improve policy    \n","  def policyIteration(self, initialPolicy , nIterations= np.inf):\n","    assert isinstance(initialPolicy, np.ndarray), \"The initial policy should be numpy arrays\"\n","    print(f'The initial policy is {initialPolicy}')\n","    n=0\n","    policy= initialPolicy\n","    newPolicy= None\n","    while n < nIterations:\n","      V = self.evaluatePolicy(policy)\n","      newPolicy = self.improvePolicy(V)\n","      print(f'iteration {n} \\n policy iteration inside loop new value : {V} \\n,new policy: {newPolicy} \\n')\n","      if(np.array_equal(policy,newPolicy)):#Convergence Criteria\n","        break\n","      policy = newPolicy\n","      n+=1\n","\n","    print(f'The final value function is {V}')\n","    return n,newPolicy\n","\n","\n","#NOTE: The goal of partial policy evaluation is to estimate the value function of a fixed policy, without changing the policy itself.(i.e the policy remains constant) \n","  def evaluatePolicyPartially(self,policy,initialV,nIterations = np.inf,tolerance=0.01,isReturnEpsilon = False):\n","    newT = self.getTransitionForPolicy(policy)\n","    n = 0\n","    epsilon = float('inf')\n","    V=initialV\n","    while(n<nIterations and epsilon > tolerance):\n","      newV = self.R + self.discount*(np.dot(newT,V))\n","      epsilon = np.linalg.norm(newV - V)\n","      V= newV\n","      n+=1\n","    if( not isReturnEpsilon):\n","      return V\n","    return V,epsilon\n","\n","\n","\n","  def modifiedPolicyIteration(self,initialPolicy,initialV,nEvalIterations=5,nIterations = np.inf,tolerance=0.01):\n","    #main point: performs approximate policy evaluation using value iteration and then performs approximate policy improvement to obtain a better policy based on the approximate value function. \n","    #keyword APPROXIMATE\n","    policy=initialPolicy\n","    V= np.zeros(self.R.shape)\n","    n=0\n","    newV = None\n","    epsilon = float('inf')\n","    while(n<nIterations and epsilon>tolerance):\n","      print(\"Inside\")\n","      #Question: If epsilon continues to be less than 0.01, but the policy are not same, the the value of V wont change?\n","      newV,epsilon = self.evaluatePolicyPartially(policy,V,nEvalIterations,tolerance,True)\n","      newPolicy = self.improvePolicy(V)\n","      epsilon = np.linalg.norm(newV - V) #Convergence Criteria\n","      #ask\n","      V = newV\n","      policy = newPolicy\n","      n+=1\n","    return policy,n,epsilon\n","\n","\n"]},{"cell_type":"code","source":["#testing\n","\n","#initialization\n","T_A = np.array([[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]])\n","T_S = np.array([[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]])\n","T = np.array([T_A, T_S])\n","discount=0.9\n","R = np.array([[0],[0],[10],[10]])\n","initalV = np.zeros(R.shape)\n","mdp = MDP(T,R,discount)\n","\n","#for policy iteration initial policy pi(PU) = A pi(PF)= A pi(RU)=A pi(RF)=A\n","initialPolicy= np.array([[0],[0],[0],[0]])"],"metadata":{"id":"yCn5HLgKYTGn","executionInfo":{"status":"ok","timestamp":1677468109334,"user_tz":360,"elapsed":145,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#functions\n","# mdp.valueIteration(initalV,80,0.01)\n","\n","mdp.policyIteration(initialPolicy,50)\n","\n","# answers\n","# policyIteration #3\n","# partially #2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7EyJHmp7ZPg","executionInfo":{"status":"ok","timestamp":1677468111306,"user_tz":360,"elapsed":161,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"b5406d74-9b7c-4df0-d5da-2602aec8b174"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The initial policy is [[0]\n"," [0]\n"," [0]\n"," [0]]\n","iteration 0 \n"," policy iteration inside loop new value : [[ 0.]\n"," [ 0.]\n"," [10.]\n"," [10.]] \n",",new policy: [[0]\n"," [1]\n"," [1]\n"," [1]] \n","\n","iteration 1 \n"," policy iteration inside loop new value : [[31.58510431]\n"," [38.60401638]\n"," [44.02417625]\n"," [54.20159875]] \n",",new policy: [[0]\n"," [1]\n"," [1]\n"," [1]] \n","\n","The final value function is [[31.58510431]\n"," [38.60401638]\n"," [44.02417625]\n"," [54.20159875]]\n"]},{"output_type":"execute_result","data":{"text/plain":["(1, array([[0],\n","        [1],\n","        [1],\n","        [1]]))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Questions\n","#1. Report the policy, value function, and the number of iterations needed by value iteration when\n","# using a tolerance of 0.01 and starting from a value function set to 0 for all states\n","initialV = np.zeros(R.shape)\n","newV,n,epsilon = mdp.valueIteration(initialV,80,0.01)\n","policy= mdp.extractPolicy(newV)\n","print(f'the value iteration final value function = {newV}\\n number of iterations ={n} \\n value of epsilon = {epsilon}\\n the policy = {policy}')\n","\n","#2. Report the policy, value function, and the number of iterations needed by policy iteration to find\n","# an optimal policy when starting from the policy that chooses action 0 in all states.\n","# Note: action 0 corresponds to “A: Advertising” whereas action 1 corresponds to “S: Saving\n","# money”\n","initialPolicy= np.array([[0],[0],[0],[0]])\n","n,newPolicy = mdp.policyIteration(initialPolicy,50)\n","print(f'the final policy iteration  number of iterations ={n} \\n ,optimal policy: {newPolicy}  \\n')\n","\n","# 3.Report the number of iterations needed by modified policy iteration to converge when varying\n","# the number of iterations in partial policy evaluation from 1 to 10. Use a tolerance of 0.01, start\n","# with the policy that chooses action 0 in all states and start with the value function that assigns 0\n","# to all states.\n","\n","for value in range(1,11):\n","  policy,n,epsilon= mdp.modifiedPolicyIteration(initialPolicy,initialV,nEvalIterations=value,nIterations = 100,tolerance=0.01)\n","  print(f'For modified policy iteration with number_of_iteration_in_partial_policy = {value}\\n  number of iterations required to converge ={n} \\n optimal policy: {policy}  \\n')\n","\n","\n","# 4.Discuss the impact of the number of iterations in partial policy evaluation on the results and relate the results to value iteration and policy iteration\n","In this case, the number of partial iteration \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUlRabTdXOTT","executionInfo":{"status":"ok","timestamp":1677468143080,"user_tz":360,"elapsed":147,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"40daebc2-5194-4a1b-f73f-0678d0dc1056"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 1 \n"," value iteration inside loop new value : [[[ 0.]\n","  [ 0.]\n","  [10.]\n","  [10.]]\n","\n"," [[ 0.]\n","  [ 0.]\n","  [10.]\n","  [10.]]] \n",", max value: [[ 0.]\n"," [ 0.]\n"," [10.]\n"," [10.]] \n",", epsilon: 14.142135623730951\n","iteration 2 \n"," value iteration inside loop new value : [[[ 0. ]\n","  [ 0. ]\n","  [10. ]\n","  [10. ]]\n","\n"," [[ 0. ]\n","  [ 4.5]\n","  [14.5]\n","  [19. ]]] \n",", max value: [[ 0. ]\n"," [ 4.5]\n"," [14.5]\n"," [19. ]] \n",", epsilon: 11.022703842524301\n","iteration 3 \n"," value iteration inside loop new value : [[[ 2.025]\n","  [ 4.05 ]\n","  [12.025]\n","  [14.05 ]]\n","\n"," [[ 0.   ]\n","  [ 8.55 ]\n","  [16.525]\n","  [25.075]]] \n",", max value: [[ 2.025]\n"," [ 8.55 ]\n"," [16.525]\n"," [25.075]] \n",", epsilon: 7.842791276070021\n","iteration 4 \n"," value iteration inside loop new value : [[[ 4.75875]\n","  [ 7.695  ]\n","  [14.75875]\n","  [17.695  ]]\n","\n"," [[ 1.8225 ]\n","  [12.195  ]\n","  [18.3475 ]\n","  [28.72   ]]] \n",", max value: [[ 4.75875]\n"," [12.195  ]\n"," [18.3475 ]\n"," [28.72   ]] \n",", epsilon: 6.112850833490049\n","iteration 5 \n"," value iteration inside loop new value : [[[ 7.6291875]\n","  [10.9755   ]\n","  [17.6291875]\n","  [20.9755   ]]\n","\n"," [[ 4.282875 ]\n","  [15.0654375]\n","  [20.3978125]\n","  [31.180375 ]]] \n",", max value: [[ 7.6291875]\n"," [15.0654375]\n"," [20.3978125]\n"," [31.180375 ]] \n",", epsilon: 5.170691382309889\n","iteration 6 \n"," value iteration inside loop new value : [[[10.21258125]\n","  [13.55889375]\n","  [20.21258125]\n","  [23.55889375]]\n","\n"," [[ 6.86626875]\n","  [17.46430313]\n","  [22.61215   ]\n","  [33.21018437]]] \n",", max value: [[10.21258125]\n"," [17.46430313]\n"," [22.61215   ]\n"," [33.21018437]] \n",", epsilon: 4.631619178763495\n","iteration 7 \n"," value iteration inside loop new value : [[[12.45459797]\n","  [15.71787281]\n","  [22.45459797]\n","  [25.71787281]]\n","\n"," [[ 9.19132313]\n","  [19.54024453]\n","  [24.77112906]\n","  [35.12005047]]] \n",", max value: [[12.45459797]\n"," [19.54024453]\n"," [24.77112906]\n"," [35.12005047]] \n",", epsilon: 4.200589336951067\n","iteration 8 \n"," value iteration inside loop new value : [[[14.39767912]\n","  [17.58622008]\n","  [24.39767912]\n","  [27.58622008]]\n","\n"," [[11.20913817]\n","  [21.4085918 ]\n","  [26.75157716]\n","  [36.95103079]]] \n",", max value: [[14.39767912]\n"," [21.4085918 ]\n"," [26.75157716]\n"," [36.95103079]] \n",", epsilon: 3.813259694945578\n","iteration 9 \n"," value iteration inside loop new value : [[[16.11282191]\n","  [19.26773262]\n","  [26.11282191]\n","  [29.26773262]]\n","\n"," [[12.95791121]\n","  [23.10691946]\n","  [28.51716533]\n","  [38.66617358]]] \n",", max value: [[16.11282191]\n"," [23.10691946]\n"," [28.51716533]\n"," [38.66617358]] \n",", epsilon: 3.4474697978266975\n","iteration 10 \n"," value iteration inside loop new value : [[[17.64888362]\n","  [20.79622752]\n","  [27.64888362]\n","  [30.79622752]]\n","\n"," [[14.50153972]\n","  [24.65054797]\n","  [30.08349426]\n","  [40.23250251]]] \n",", max value: [[17.64888362]\n"," [24.65054797]\n"," [30.08349426]\n"," [40.23250251]] \n",", epsilon: 3.106291546866111\n","iteration 11 \n"," value iteration inside loop new value : [[[19.03474422]\n","  [22.18549317]\n","  [29.03474422]\n","  [32.18549317]]\n","\n"," [[15.88399526]\n","  [26.04662376]\n","  [31.47957005]\n","  [41.64219855]]] \n",", max value: [[19.03474422]\n"," [26.04662376]\n"," [31.47957005]\n"," [41.64219855]] \n",", epsilon: 2.7939054581544607\n","iteration 12 \n"," value iteration inside loop new value : [[[20.28661559]\n","  [23.44196138]\n","  [30.28661559]\n","  [33.44196138]]\n","\n"," [[17.13126979]\n","  [27.30462424]\n","  [32.73144142]\n","  [42.90479587]]] \n",", max value: [[20.28661559]\n"," [27.30462424]\n"," [32.73144142]\n"," [42.90479587]] \n",", epsilon: 2.5121865134908288\n","iteration 13 \n"," value iteration inside loop new value : [[[21.41605792]\n","  [24.57416182]\n","  [31.41605792]\n","  [34.57416182]]\n","\n"," [[18.25795403]\n","  [28.43613515]\n","  [33.85812565]\n","  [44.03630678]]] \n",", max value: [[21.41605792]\n"," [28.43613515]\n"," [33.85812565]\n"," [44.03630678]] \n",", epsilon: 2.2595776687161844\n","iteration 14 \n"," value iteration inside loop new value : [[[22.43348689]\n","  [25.59252164]\n","  [32.43348689]\n","  [35.59252164]]\n","\n"," [[19.27445213]\n","  [29.45356412]\n","  [34.87338261]\n","  [45.05249459]]] \n",", max value: [[22.43348689]\n"," [29.45356412]\n"," [34.87338261]\n"," [45.05249459]] \n",", epsilon: 2.033152170445249\n","iteration 15 \n"," value iteration inside loop new value : [[[23.34917295]\n","  [26.5082077 ]\n","  [33.34917295]\n","  [36.5082077 ]]\n","\n"," [[20.1901382 ]\n","  [30.36869167]\n","  [35.78809127]\n","  [45.96664474]]] \n",", max value: [[23.34917295]\n"," [30.36869167]\n"," [35.78809127]\n"," [45.96664474]] \n",", epsilon: 1.8298365591797214\n","iteration 16 \n"," value iteration inside loop new value : [[[24.17303908]\n","  [27.3318225 ]\n","  [34.17303908]\n","  [37.3318225 ]]\n","\n"," [[21.01425566]\n","  [31.19211796]\n","  [36.6117689 ]\n","  [46.78963121]]] \n",", max value: [[24.17303908]\n"," [31.19211796]\n"," [36.6117689 ]\n"," [46.78963121]] \n",", epsilon: 1.6469783894290957\n","iteration 17 \n"," value iteration inside loop new value : [[[24.91432067]\n","  [28.07290617]\n","  [34.91432067]\n","  [38.07290617]]\n","\n"," [[21.75573517]\n","  [31.93320163]\n","  [37.35316359]\n","  [47.53063005]]] \n",", max value: [[24.91432067]\n"," [31.93320163]\n"," [37.35316359]\n"," [47.53063005]] \n",", epsilon: 1.4823794268843746\n","iteration 18 \n"," value iteration inside loop new value : [[[25.58138503]\n","  [28.73988146]\n","  [35.58138503]\n","  [38.73988146]]\n","\n"," [[22.4228886 ]\n","  [32.60022782]\n","  [38.02036792]\n","  [48.19770714]]] \n",", max value: [[25.58138503]\n"," [32.60022782]\n"," [38.02036792]\n"," [48.19770714]] \n",", epsilon: 1.3341859940045109\n","iteration 19 \n"," value iteration inside loop new value : [[[26.18172578]\n","  [29.34020504]\n","  [36.18172578]\n","  [39.34020504]]\n","\n"," [[23.02324653]\n","  [33.20059148]\n","  [38.62078883]\n","  [48.79813377]]] \n",", max value: [[26.18172578]\n"," [33.20059148]\n"," [38.62078883]\n"," [48.79813377]] \n",", epsilon: 1.200775979289596\n","iteration 20 \n"," value iteration inside loop new value : [[[26.72204277]\n","  [29.88053233]\n","  [36.72204277]\n","  [39.88053233]]\n","\n"," [[23.56355321]\n","  [33.7409368 ]\n","  [39.16113158]\n","  [49.33851517]]] \n",", max value: [[26.72204277]\n"," [33.7409368 ]\n"," [39.16113158]\n"," [49.33851517]] \n",", epsilon: 1.0806932272198566\n","iteration 21 \n"," value iteration inside loop new value : [[[27.20834081]\n","  [30.36684312]\n","  [37.20834081]\n","  [40.36684312]]\n","\n"," [[24.04983849]\n","  [34.22725107]\n","  [39.64742845]\n","  [49.82484104]]] \n",", max value: [[27.20834081]\n"," [34.22725107]\n"," [39.64742845]\n"," [49.82484104]] \n",", epsilon: 0.9726175269699581\n","iteration 22 \n"," value iteration inside loop new value : [[[27.64601635]\n","  [30.80452596]\n","  [37.64601635]\n","  [40.80452596]]\n","\n"," [[24.48750673]\n","  [34.66493183]\n","  [40.08509617]\n","  [50.26252127]]] \n",", max value: [[27.64601635]\n"," [34.66493183]\n"," [40.08509617]\n"," [50.26252127]] \n",", epsilon: 0.8753521218147713\n","iteration 23 \n"," value iteration inside loop new value : [[[28.03992668]\n","  [31.19843865]\n","  [38.03992668]\n","  [41.19843865]]\n","\n"," [[24.88141471]\n","  [35.05884193]\n","  [40.47900063]\n","  [50.65642785]]] \n",", max value: [[28.03992668]\n"," [35.05884193]\n"," [40.47900063]\n"," [50.65642785]] \n",", epsilon: 0.7878157356540653\n","iteration 24 \n"," value iteration inside loop new value : [[[28.39444587]\n","  [31.55295773]\n","  [38.39444587]\n","  [41.55295773]]\n","\n"," [[25.23593401]\n","  [35.41335954]\n","  [40.83351729]\n","  [51.01094281]]] \n",", max value: [[28.39444587]\n"," [35.41335954]\n"," [40.83351729]\n"," [51.01094281]] \n",", epsilon: 0.7090342149086175\n","iteration 25 \n"," value iteration inside loop new value : [[[28.71351243]\n","  [31.87202358]\n","  [38.71351243]\n","  [41.87202358]]\n","\n"," [[25.55500129]\n","  [35.73242491]\n","  [41.15258342]\n","  [51.33000705]]] \n",", max value: [[28.71351243]\n"," [35.73242491]\n"," [41.15258342]\n"," [51.33000705]] \n",", epsilon: 0.6381311499980432\n","iteration 26 \n"," value iteration inside loop new value : [[[29.0006718 ]\n","  [32.15918242]\n","  [39.0006718 ]\n","  [42.15918242]]\n","\n"," [[25.84216119]\n","  [36.01958377]\n","  [41.43974314]\n","  [51.61716571]]] \n",", max value: [[29.0006718 ]\n"," [36.01958377]\n"," [41.43974314]\n"," [51.61716571]] \n",", epsilon: 0.5743183024344697\n","iteration 27 \n"," value iteration inside loop new value : [[[29.25911501]\n","  [32.41762539]\n","  [39.25911501]\n","  [42.41762539]]\n","\n"," [[26.10060462]\n","  [36.27802688]\n","  [41.69818672]\n","  [51.87560898]]] \n",", max value: [[29.25911501]\n"," [36.27802688]\n"," [41.69818672]\n"," [51.87560898]] \n",", epsilon: 0.5168865877237538\n","iteration 28 \n"," value iteration inside loop new value : [[[29.49171385]\n","  [32.65022419]\n","  [39.49171385]\n","  [42.65022419]]\n","\n"," [[26.33320351]\n","  [36.51062579]\n","  [41.93078578]\n","  [52.10820807]]] \n",", max value: [[29.49171385]\n"," [36.51062579]\n"," [41.93078578]\n"," [52.10820807]] \n",", epsilon: 0.46519794844752343\n","iteration 29 \n"," value iteration inside loop new value : [[[29.70105284]\n","  [32.85956322]\n","  [39.70105284]\n","  [42.85956322]]\n","\n"," [[26.54254246]\n","  [36.71996486]\n","  [42.14012483]\n","  [52.31754723]]] \n",", max value: [[29.70105284]\n"," [36.71996486]\n"," [42.14012483]\n"," [52.31754723]] \n",", epsilon: 0.4186781380057833\n","iteration 30 \n"," value iteration inside loop new value : [[[29.88945797]\n","  [33.04796838]\n","  [39.88945797]\n","  [43.04796838]]\n","\n"," [[26.73094756]\n","  [36.90837003]\n","  [42.32852995]\n","  [52.50595243]]] \n",", max value: [[29.88945797]\n"," [36.90837003]\n"," [42.32852995]\n"," [52.50595243]] \n",", epsilon: 0.3768103066585927\n","iteration 31 \n"," value iteration inside loop new value : [[[30.0590226 ]\n","  [33.21753303]\n","  [40.0590226 ]\n","  [43.21753303]]\n","\n"," [[26.90051217]\n","  [37.07793468]\n","  [42.49809456]\n","  [52.67551707]]] \n",", max value: [[30.0590226 ]\n"," [37.07793468]\n"," [42.49809456]\n"," [52.67551707]] \n",", epsilon: 0.3391292663201686\n","iteration 32 \n"," value iteration inside loop new value : [[[30.21163077]\n","  [33.37014121]\n","  [40.21163077]\n","  [43.37014121]]\n","\n"," [[27.05312034]\n","  [37.23054285]\n","  [42.65070272]\n","  [52.82812524]]] \n",", max value: [[30.21163077]\n"," [37.23054285]\n"," [42.65070272]\n"," [52.82812524]] \n",", epsilon: 0.3052163367567771\n","iteration 33 \n"," value iteration inside loop new value : [[[30.34897813]\n","  [33.50748857]\n","  [40.34897813]\n","  [43.50748857]]\n","\n"," [[27.1904677 ]\n","  [37.3678902 ]\n","  [42.78805007]\n","  [52.96547258]]] \n",", max value: [[30.34897813]\n"," [37.3678902 ]\n"," [42.78805007]\n"," [52.96547258]] \n",", epsilon: 0.27469470336090585\n","iteration 34 \n"," value iteration inside loop new value : [[[30.47259075]\n","  [33.63110118]\n","  [40.47259075]\n","  [43.63110118]]\n","\n"," [[27.31408032]\n","  [37.49150282]\n","  [42.91166269]\n","  [53.08908519]]] \n",", max value: [[30.47259075]\n"," [37.49150282]\n"," [42.91166269]\n"," [53.08908519]] \n",", epsilon: 0.24722523403214505\n","iteration 35 \n"," value iteration inside loop new value : [[[30.58384211]\n","  [33.74235254]\n","  [40.58384211]\n","  [43.74235254]]\n","\n"," [[27.42533168]\n","  [37.60275418]\n","  [43.02291405]\n","  [53.20033655]]] \n",", max value: [[30.58384211]\n"," [37.60275418]\n"," [43.02291405]\n"," [53.20033655]] \n",", epsilon: 0.22250271134936206\n","iteration 36 \n"," value iteration inside loop new value : [[[30.68396833]\n","  [33.84247876]\n","  [40.68396833]\n","  [43.84247876]]\n","\n"," [[27.5254579 ]\n","  [37.7028804 ]\n","  [43.12304027]\n","  [53.30046277]]] \n",", max value: [[30.68396833]\n"," [37.7028804 ]\n"," [43.12304027]\n"," [53.30046277]] \n",", epsilon: 0.20025244051310942\n","iteration 37 \n"," value iteration inside loop new value : [[[30.77408193]\n","  [33.93259236]\n","  [40.77408193]\n","  [43.93259236]]\n","\n"," [[27.61557149]\n","  [37.79299399]\n","  [43.21315387]\n","  [53.39057637]]] \n",", max value: [[30.77408193]\n"," [37.79299399]\n"," [43.21315387]\n"," [53.39057637]] \n",", epsilon: 0.1802271965044131\n","iteration 38 \n"," value iteration inside loop new value : [[[30.85518416]\n","  [34.01369459]\n","  [40.85518416]\n","  [44.01369459]]\n","\n"," [[27.69667373]\n","  [37.87409623]\n","  [43.29425611]\n","  [53.47167861]]] \n",", max value: [[30.85518416]\n"," [37.87409623]\n"," [43.29425611]\n"," [53.47167861]] \n",", epsilon: 0.1622044768075046\n","iteration 39 \n"," value iteration inside loop new value : [[[30.92817618]\n","  [34.08668661]\n","  [40.92817618]\n","  [44.08668661]]\n","\n"," [[27.76966575]\n","  [37.94708825]\n","  [43.36724812]\n","  [53.54467062]]] \n",", max value: [[30.92817618]\n"," [37.94708825]\n"," [43.36724812]\n"," [53.54467062]] \n",", epsilon: 0.1459840290786225\n","iteration 40 \n"," value iteration inside loop new value : [[[30.99386899]\n","  [34.15237942]\n","  [40.99386899]\n","  [44.15237942]]\n","\n"," [[27.83535856]\n","  [38.01278106]\n","  [43.43294094]\n","  [53.61036343]]] \n",", max value: [[30.99386899]\n"," [38.01278106]\n"," [43.43294094]\n"," [53.61036343]] \n",", epsilon: 0.13138562614522087\n","iteration 41 \n"," value iteration inside loop new value : [[[31.05299252]\n","  [34.21150295]\n","  [41.05299252]\n","  [44.21150295]]\n","\n"," [[27.89448209]\n","  [38.07190459]\n","  [43.49206447]\n","  [53.66948697]]] \n",", max value: [[31.05299252]\n"," [38.07190459]\n"," [43.49206447]\n"," [53.66948697]] \n",", epsilon: 0.11824706352343384\n","iteration 42 \n"," value iteration inside loop new value : [[[31.1062037 ]\n","  [34.26471413]\n","  [41.1062037 ]\n","  [44.26471413]]\n","\n"," [[27.94769327]\n","  [38.12511577]\n","  [43.54527565]\n","  [53.72269814]]] \n",", max value: [[31.1062037 ]\n"," [38.12511577]\n"," [43.54527565]\n"," [53.72269814]] \n",", epsilon: 0.10642235717221027\n","iteration 43 \n"," value iteration inside loop new value : [[[31.15409376]\n","  [34.31260419]\n","  [41.15409376]\n","  [44.31260419]]\n","\n"," [[27.99558333]\n","  [38.17300583]\n","  [43.59316571]\n","  [53.77058821]]] \n",", max value: [[31.15409376]\n"," [38.17300583]\n"," [43.59316571]\n"," [53.77058821]] \n",", epsilon: 0.09578012145782112\n","iteration 44 \n"," value iteration inside loop new value : [[[31.19719482]\n","  [34.35570525]\n","  [41.19719482]\n","  [44.35570525]]\n","\n"," [[28.03868439]\n","  [38.21610689]\n","  [43.63626676]\n","  [53.81368926]]] \n",", max value: [[31.19719482]\n"," [38.21610689]\n"," [43.63626676]\n"," [53.81368926]] \n",", epsilon: 0.08620210931397843\n","iteration 45 \n"," value iteration inside loop new value : [[[31.23598577]\n","  [34.3944962 ]\n","  [41.23598577]\n","  [44.3944962 ]]\n","\n"," [[28.07747534]\n","  [38.25489783]\n","  [43.67505771]\n","  [53.85248021]]] \n",", max value: [[31.23598577]\n"," [38.25489783]\n"," [43.67505771]\n"," [53.85248021]] \n",", epsilon: 0.0775818983833485\n","iteration 46 \n"," value iteration inside loop new value : [[[31.27089762]\n","  [34.42940805]\n","  [41.27089762]\n","  [44.42940805]]\n","\n"," [[28.11238719]\n","  [38.28980969]\n","  [43.70996956]\n","  [53.88739206]]] \n",", max value: [[31.27089762]\n"," [38.28980969]\n"," [43.70996956]\n"," [53.88739206]] \n",", epsilon: 0.069823708545103\n","iteration 47 \n"," value iteration inside loop new value : [[[31.30231829]\n","  [34.46082872]\n","  [41.30231829]\n","  [44.46082872]]\n","\n"," [[28.14380786]\n","  [38.32123036]\n","  [43.74139023]\n","  [53.91881273]]] \n",", max value: [[31.30231829]\n"," [38.32123036]\n"," [43.74139023]\n"," [53.91881273]] \n",", epsilon: 0.06284133769045397\n","iteration 48 \n"," value iteration inside loop new value : [[[31.33059689]\n","  [34.48910732]\n","  [41.33059689]\n","  [44.48910732]]\n","\n"," [[28.17208646]\n","  [38.34950896]\n","  [43.76966884]\n","  [53.94709133]]] \n",", max value: [[31.33059689]\n"," [38.34950896]\n"," [43.76966884]\n"," [53.94709133]] \n",", epsilon: 0.05655720392127428\n","iteration 49 \n"," value iteration inside loop new value : [[[31.35604763]\n","  [34.51455806]\n","  [41.35604763]\n","  [44.51455806]]\n","\n"," [[28.1975372 ]\n","  [38.3749597 ]\n","  [43.79511958]\n","  [53.97254208]]] \n",", max value: [[31.35604763]\n"," [38.3749597 ]\n"," [43.79511958]\n"," [53.97254208]] \n",", epsilon: 0.050901483529081304\n","iteration 50 \n"," value iteration inside loop new value : [[[31.3789533 ]\n","  [34.53746373]\n","  [41.3789533 ]\n","  [44.53746373]]\n","\n"," [[28.22044287]\n","  [38.39786537]\n","  [43.81802524]\n","  [53.99544774]]] \n",", max value: [[31.3789533 ]\n"," [38.39786537]\n"," [43.81802524]\n"," [53.99544774]] \n",", epsilon: 0.04581133517615932\n","iteration 51 \n"," value iteration inside loop new value : [[[31.3995684 ]\n","  [34.55807883]\n","  [41.3995684 ]\n","  [44.55807883]]\n","\n"," [[28.24105797]\n","  [38.41848047]\n","  [43.83864035]\n","  [54.01606284]]] \n",", max value: [[31.3995684 ]\n"," [38.41848047]\n"," [43.83864035]\n"," [54.01606284]] \n",", epsilon: 0.04123020165854463\n","iteration 52 \n"," value iteration inside loop new value : [[[31.41812199]\n","  [34.57663242]\n","  [41.41812199]\n","  [44.57663242]]\n","\n"," [[28.25961156]\n","  [38.43703406]\n","  [43.85719394]\n","  [54.03461644]]] \n",", max value: [[31.41812199]\n"," [38.43703406]\n"," [43.85719394]\n"," [54.03461644]] \n",", epsilon: 0.03710718149270065\n","iteration 53 \n"," value iteration inside loop new value : [[[31.43482022]\n","  [34.59333065]\n","  [41.43482022]\n","  [44.59333065]]\n","\n"," [[28.27630979]\n","  [38.45373229]\n","  [43.87389217]\n","  [54.05131467]]] \n",", max value: [[31.43482022]\n"," [38.45373229]\n"," [43.87389217]\n"," [54.05131467]] \n",", epsilon: 0.03339646334343094\n","iteration 54 \n"," value iteration inside loop new value : [[[31.44984863]\n","  [34.60835906]\n","  [41.44984863]\n","  [44.60835906]]\n","\n"," [[28.2913382 ]\n","  [38.4687607 ]\n","  [43.88892058]\n","  [54.06634308]]] \n",", max value: [[31.44984863]\n"," [38.4687607 ]\n"," [43.88892058]\n"," [54.06634308]] \n",", epsilon: 0.03005681700909335\n","iteration 55 \n"," value iteration inside loop new value : [[[31.4633742 ]\n","  [34.62188463]\n","  [41.4633742 ]\n","  [44.62188463]]\n","\n"," [[28.30486377]\n","  [38.48228627]\n","  [43.90244614]\n","  [54.07986864]]] \n",", max value: [[31.4633742 ]\n"," [38.48228627]\n"," [43.90244614]\n"," [54.07986864]] \n",", epsilon: 0.02705113530818437\n","iteration 56 \n"," value iteration inside loop new value : [[[31.47554721]\n","  [34.63405764]\n","  [41.47554721]\n","  [44.63405764]]\n","\n"," [[28.31703678]\n","  [38.49445928]\n","  [43.91461915]\n","  [54.09204165]]] \n",", max value: [[31.47554721]\n"," [38.49445928]\n"," [43.91461915]\n"," [54.09204165]] \n",", epsilon: 0.024346021777363447\n","iteration 57 \n"," value iteration inside loop new value : [[[31.48650292]\n","  [34.64501335]\n","  [41.48650292]\n","  [44.64501335]]\n","\n"," [[28.32799249]\n","  [38.50541499]\n","  [43.92557486]\n","  [54.10299736]]] \n",", max value: [[31.48650292]\n"," [38.50541499]\n"," [43.92557486]\n"," [54.10299736]] \n",", epsilon: 0.02191141959962728\n","iteration 58 \n"," value iteration inside loop new value : [[[31.49636306]\n","  [34.65487349]\n","  [41.49636306]\n","  [44.65487349]]\n","\n"," [[28.33785263]\n","  [38.51527513]\n","  [43.935435  ]\n","  [54.1128575 ]]] \n",", max value: [[31.49636306]\n"," [38.51527513]\n"," [43.935435  ]\n"," [54.1128575 ]] \n",", epsilon: 0.01972027763966544\n","iteration 59 \n"," value iteration inside loop new value : [[[31.50523718]\n","  [34.66374762]\n","  [41.50523718]\n","  [44.66374762]]\n","\n"," [[28.34672675]\n","  [38.52414925]\n","  [43.94430913]\n","  [54.12173163]]] \n",", max value: [[31.50523718]\n"," [38.52414925]\n"," [43.94430913]\n"," [54.12173163]] \n",", epsilon: 0.017748249875690902\n","iteration 60 \n"," value iteration inside loop new value : [[[31.5132239 ]\n","  [34.67173433]\n","  [41.5132239 ]\n","  [44.67173433]]\n","\n"," [[28.35471347]\n","  [38.53213597]\n","  [43.95229584]\n","  [54.12971834]]] \n",", max value: [[31.5132239 ]\n"," [38.53213597]\n"," [43.95229584]\n"," [54.12971834]] \n",", epsilon: 0.01597342488813247\n","iteration 61 \n"," value iteration inside loop new value : [[[31.52041194]\n","  [34.67892237]\n","  [41.52041194]\n","  [44.67892237]]\n","\n"," [[28.36190151]\n","  [38.53932401]\n","  [43.95948388]\n","  [54.13690638]]] \n",", max value: [[31.52041194]\n"," [38.53932401]\n"," [43.95948388]\n"," [54.13690638]] \n",", epsilon: 0.014376082399319756\n","iteration 62 \n"," value iteration inside loop new value : [[[31.52688118]\n","  [34.68539161]\n","  [41.52688118]\n","  [44.68539161]]\n","\n"," [[28.36837074]\n","  [38.54579324]\n","  [43.96595312]\n","  [54.14337562]]] \n",", max value: [[31.52688118]\n"," [38.54579324]\n"," [43.96595312]\n"," [54.14337562]] \n",", epsilon: 0.012938474159382096\n","iteration 63 \n"," value iteration inside loop new value : [[[31.53270349]\n","  [34.69121392]\n","  [41.53270349]\n","  [44.69121392]]\n","\n"," [[28.37419306]\n","  [38.55161556]\n","  [43.97177543]\n","  [54.14919793]]] \n",", max value: [[31.53270349]\n"," [38.55161556]\n"," [43.97177543]\n"," [54.14919793]] \n",", epsilon: 0.01164462674344513\n","iteration 64 \n"," value iteration inside loop new value : [[[31.53794357]\n","  [34.696454  ]\n","  [41.53794357]\n","  [44.696454  ]]\n","\n"," [[28.37943314]\n","  [38.55685564]\n","  [43.97701551]\n","  [54.15443801]]] \n",", max value: [[31.53794357]\n"," [38.55685564]\n"," [43.97701551]\n"," [54.15443801]] \n",", epsilon: 0.010480164069100795\n","iteration 65 \n"," value iteration inside loop new value : [[[31.54265964]\n","  [34.70117008]\n","  [41.54265964]\n","  [44.70117008]]\n","\n"," [[28.38414921]\n","  [38.56157171]\n","  [43.98173159]\n","  [54.15915409]]] \n",", max value: [[31.54265964]\n"," [38.56157171]\n"," [43.98173159]\n"," [54.15915409]] \n",", epsilon: 0.009432147662192136\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 1\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 2\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 3\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 4\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 5\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 6\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 7\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 8\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 9\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n","Inside\n","Inside\n","For modified policy iteration with number of iteration in partial policy = 10\n","  number of iterations required to converge =2 \n"," optimal policy: [[0]\n"," [1]\n"," [1]\n"," [1]]  \n","\n"]}]},{"cell_type":"markdown","source":["NOTES:\n","Policy Iteration vs Modified Policy Iteration\n","**bold text**\n","\n","The main difference between normal policy iteration and modified policy iteration lies in how they perform policy evaluation and policy improvement.\n","\n","In normal policy iteration, the algorithm performs exact policy evaluation to determine the value function for a given policy, and then performs exact policy improvement to obtain a better policy based on the value function. This process is repeated until the policy converges to an optimal policy. Exact policy evaluation involves solving a set of linear equations or matrix inversion, which can be computationally expensive, especially for large problems.\n","\n","In modified policy iteration, the algorithm performs approximate policy evaluation using value iteration, and then performs approximate policy improvement to obtain a better policy based on the approximate value function. This process is repeated until the policy converges to an optimal policy. Value iteration involves iteratively updating the value function using the Bellman equation, which is less computationally expensive than exact policy evaluation.\n","\n","The advantage of modified policy iteration over normal policy iteration is that it can converge to an optimal policy faster, especially for large problems, because it avoids the computational expense of exact policy evaluation. However, the policy obtained by modified policy iteration may not be as accurate as the one obtained by normal policy iteration because it relies on approximations.\n","\n","Partial Policy Iteration\n","**bold text**\n","In reinforcement learning, partial policy evaluation refers to the process of estimating the value function of a policy based on incomplete information. Specifically, it involves estimating the value function of a policy for some states, without necessarily having information about the policy for all states.\n","\n","One common approach to partial policy evaluation is to use the Bellman equation, which relates the value function of a state to the value function of its successor states. By recursively applying the Bellman equation, we can estimate the value function for a set of states, even if we do not have complete information about the policy.\n","\n","THE POLICY DURING POLICY ITERATION DOES NOT CHANGE. The goal of partial policy evaluation is to estimate the value function of a fixed policy, without changing the policy itself.\n","\n","\n","Identity Matrix\n","**bold text**\n","An identity matrix is a special type of square matrix in linear algebra that has ones along the diagonal and zeros elsewhere. It is denoted by the symbol \"I\" and has the property that when it is multiplied by any square matrix of the same size, the result is the original matrix.\n","\n","For example, the 2x2 identity matrix is:\n","\n","\n","I = [[1, 0],\n","     [0, 1]]\n","When this matrix is multiplied by any 2x2 matrix A, the result is simply A:\n","\n","\n","I * A = A * I = A"],"metadata":{"id":"fda4o3Y3jI-8"}},{"cell_type":"code","source":["tt = np.array([[1],[2],[3]])\n","arr1 = np.array([[1, 2], [3, 4]])\n","arr2 = np.array([[10, 20], [30, 40]])\n","temp=np.zeros((arr2.shape))\n","print(f'test {arr2.shape}')\n","print(f'shape is {temp.shape}, temp value is {temp}')\n","\n","I = np.zeros((4,1))\n","T_A = np.array([[0.5,0.5,0,0],[0,1,0,0],[0,1,0,0],[0.5,0.5,0,0]])\n","print(f'{I} \\n {T_A} \\n {I-T_A}')\n","\n","# no axis provided, array elements will be flattened\n","arr_flat = np.append(arr1, arr2,axis=1)\n","\n","test= np.ones(tt.shape)\n","# print(f'{test} and {test.shape}')  # [ 1  2  3  4 10 20 30 40]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbO69Zfa8ehX","executionInfo":{"status":"ok","timestamp":1677192034087,"user_tz":360,"elapsed":6,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"86ded02c-1443-4626-d4db-d911dc93c158"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test (2, 2)\n","shape is (2, 2), temp value is [[0. 0.]\n"," [0. 0.]]\n","[[0.]\n"," [0.]\n"," [0.]\n"," [0.]] \n"," [[0.5 0.5 0.  0. ]\n"," [0.  1.  0.  0. ]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]] \n"," [[-0.5 -0.5  0.   0. ]\n"," [ 0.  -1.   0.   0. ]\n"," [ 0.  -1.   0.   0. ]\n"," [-0.5 -0.5  0.   0. ]]\n"]}]},{"cell_type":"code","source":["T_A = np.array([[0.5,0.5,0,0],[0,1,0,0],[0,1,0,0],[0.5,0.5,0,0]])\n","T_S = np.array([[1,1,0,0],[0.5,0,0.5,0],[0,0,0.5,0.5],[0.5,0,0,0.5]])\n","T = np.array([T_A, T_S])\n","temp=[]\n","print(f'Test {T[0][0]} \\n')\n","temp.append(T[0][0])\n","print(f'temp is {temp}')\n","temp.append(T[0][0])\n","print(f'temp 2 is {temp}')\n","newTemp=np.array(temp)\n","print(f'{newTemp}')\n","\n","\n","\n","\n","print(f'flatten {T_A.flatten()} \\n {T_A.flatten()} \\n {T.flatten()} ')\n","print(f'Argmax is {np.argmax(T_A)}')\n","\n","\n","print(f'New transformation is {T}')\n","discount=0.9\n","R = np.array([[0],[0],[10],[10]])\n","initialV = np.array([[0],[0],[0],[0]])\n","\n","newV = R + discount*np.dot(T,initialV)\n","A,S= newV\n","temp = np.maximum(A,S)\n","print(f' newV is {newV}')\n","\n","print(f' value is {temp}')\n","\n","policy=np.argmax(newV,axis=0)\n","print(f' policy is {policy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_e-fFvU2z8nW","executionInfo":{"status":"ok","timestamp":1677437249134,"user_tz":360,"elapsed":2,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"f4395be9-7532-4200-8674-d53930aee9a6"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Test [0.5 0.5 0.  0. ] \n","\n","temp is [array([0.5, 0.5, 0. , 0. ])]\n","temp 2 is [array([0.5, 0.5, 0. , 0. ]), array([0.5, 0.5, 0. , 0. ])]\n","[[0.5 0.5 0.  0. ]\n"," [0.5 0.5 0.  0. ]]\n","flatten [0.5 0.5 0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.5 0.5 0.  0. ] \n"," [0.5 0.5 0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.5 0.5 0.  0. ] \n"," [0.5 0.5 0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.5 0.5 0.  0.  1.  1.\n"," 0.  0.  0.5 0.  0.5 0.  0.  0.  0.5 0.5 0.5 0.  0.  0.5] \n","Argmax is 5\n","New transformation is [[[0.5 0.5 0.  0. ]\n","  [0.  1.  0.  0. ]\n","  [0.  1.  0.  0. ]\n","  [0.5 0.5 0.  0. ]]\n","\n"," [[1.  1.  0.  0. ]\n","  [0.5 0.  0.5 0. ]\n","  [0.  0.  0.5 0.5]\n","  [0.5 0.  0.  0.5]]]\n"," newV is [[[ 0.]\n","  [ 0.]\n","  [10.]\n","  [10.]]\n","\n"," [[ 0.]\n","  [ 0.]\n","  [10.]\n","  [10.]]]\n"," value is [[ 0.]\n"," [ 0.]\n"," [10.]\n"," [10.]]\n"," policy is [[0]\n"," [0]\n"," [0]\n"," [0]]\n"]}]},{"cell_type":"code","source":["for i,p in enumerate(policy):\n","  print(f'{i} {p[0]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7NnBLBB69Ek","executionInfo":{"status":"ok","timestamp":1677151829262,"user_tz":360,"elapsed":172,"user":{"displayName":"pratyush pradhan","userId":"17352935589296192647"}},"outputId":"d3aee436-ef53-4e0a-b090-a8410d81c918"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0\n","1 0\n","2 0\n","3 0\n"]}]}]}